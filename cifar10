'''
Created on 2017\10\20

@author: tingzou
'''
import sys
import os
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]="0"

import cifar10,cifar10_input
import tensorflow as tf
import numpy as np
import time

max_steps=3000
batch_size=128
data_dir='/home/qj/ZT/tmp/cifar10_data/cifar-10-batches-bin'

def variable_with_weight_loss(shape,stddev,w1):
    var=tf.Variable(tf.truncated_normal(shape, stddev=stddev))
    if w1 is not None:
        Weight_loss=tf.multiply(tf.nn.l2_loss(var), w1,name='weight_loss')
        tf.add_to_collection('losses',Weight_loss)
    return var

#download cifar10
cifar10.maybe_download_and_extract()


images_train,labels_train=cifar10_input.distorted_inputs(
                                data_dir=data_dir,batch_size=batch_size)
images_test,labels_test=cifar10_input.inputs(
                                eval_data=True, data_dir=data_dir, batch_size=batch_size)

image_holder=tf.placeholder(tf.float32, [batch_size,24,24,3])
label_holder=tf.placeholder(tf.float32,[batch_size])


weight1=variable_with_weight_loss(shape=[3,3,3,32], stddev=5e-2, w1=0.0)
kernel1=tf.nn.conv2d(image_holder,weight1,[1,1,1,1],padding='SAME')
bias1=tf.Variable(tf.constant(0.0,shape=[32]))
conv1=tf.nn.relu(tf.nn.bias_add(kernel1,bias1))
norm1=tf.nn.lrn(conv1, 4, bias=1.0, alpha=0.01/9.0, beta=0.75)
#print(norm1.shape)

weight2=variable_with_weight_loss(shape=[3,3,32,64], stddev=5e-2, w1=0.0)
kernel2=tf.nn.conv2d(norm1,weight2,[1,2,2,1],padding='SAME')
bias2=tf.Variable(tf.constant(0.1,shape=[64]))
conv2=tf.nn.relu(tf.nn.bias_add(kernel2,bias2))
norm2=tf.nn.lrn(conv2,4,bias=1.0,alpha=0.001/9.0,beta=0.75)
#print(norm2.shape)

weight3=variable_with_weight_loss(shape=[3,3,64,128], stddev=5e-2, w1=0.0)
kernel3=tf.nn.conv2d(norm2,weight3,[1,1,1,1],padding='SAME')
bias3=tf.Variable(tf.constant(0.1,shape=[128]))
conv3=tf.nn.relu(tf.nn.bias_add(kernel3,bias3))
norm3=tf.nn.lrn(conv3,4,bias=1.0,alpha=0.001/9.0,beta=0.75)
#print(norm3.shape)

weight4=variable_with_weight_loss(shape=[3,3,128,128], stddev=5e-2, w1=0.0)
kernel4=tf.nn.conv2d(norm3,weight4,[1,2,2,1],padding='SAME')
bias4=tf.Variable(tf.constant(0.1,shape=[128]))
conv4=tf.nn.relu(tf.nn.bias_add(kernel4,bias4))
norm4=tf.nn.lrn(conv4,4,bias=1.0,alpha=0.001/9.0,beta=0.75)
#print(norm4.shape)

weight5=variable_with_weight_loss(shape=[3,3,128,256], stddev=5e-2, w1=0.0)
kernel5=tf.nn.conv2d(norm4,weight5,[1,1,1,1],padding='SAME')
bias5=tf.Variable(tf.constant(0.1,shape=[256]))
conv5=tf.nn.relu(tf.nn.bias_add(kernel5,bias5))
norm5=tf.nn.lrn(conv5,4,bias=1.0,alpha=0.001/9.0,beta=0.75)
#print(norm5.shape)

weight6=variable_with_weight_loss(shape=[3,3,256,256], stddev=5e-2, w1=0.0)
kernel6=tf.nn.conv2d(norm5,weight6,[1,2,2,1],padding='SAME')
bias6=tf.Variable(tf.constant(0.1,shape=[256]))
conv6=tf.nn.relu(tf.nn.bias_add(kernel6,bias6))
norm6=tf.nn.lrn(conv6,4,bias=1.0,alpha=0.001/9.0,beta=0.75)
#print(norm5.shape)

pool=tf.nn.avg_pool(norm6, [1,3,3,1],[1,1,1,1], padding='VALID')
#print(pool)
reshape=tf.reshape(pool,[batch_size,-1])
dim=reshape.get_shape()[1].value#*
#print(dim)
weight6=variable_with_weight_loss(shape=[dim,10], stddev=0.04, w1=0.0)
bias6=tf.Variable(tf.constant(0.1,shape=[10]))
logits=tf.add(tf.matmul(reshape,weight6),bias6)




def loss(logits,labels):
    labels=tf.cast(labels,tf.int64)
    cross_entropy=tf.nn.sparse_softmax_cross_entropy_with_logits( 
                                    logits=logits,labels=labels,name='cross_entropy_per_ecample')
    cross_entropy_mean=tf.reduce_mean(cross_entropy,
                                      name='cross_entropy')
    tf.add_to_collection('losses',cross_entropy_mean)
    return tf.add_n(tf.get_collection('losses'),name='total_loss')

loss=loss(logits,label_holder)
train_op=tf.train.AdamOptimizer(1e-3).minimize(loss)
#top_k_op=tf.nn.in_top_k(logits,label_holder,1)
top_k_op=tf.nn.in_top_k(logits,tf.cast(label_holder,tf.int64), 1)

sess=tf.InteractiveSession()
tf.global_variables_initializer().run()
    
tf.train.start_queue_runners()

for step in range(max_steps):
    start_time=time.time()#time.time()
    image_batch,label_batch=sess.run([images_train,labels_train])
    _,loss_value=sess.run([train_op,loss],
        feed_dict={image_holder:image_batch,label_holder:label_batch})    
    duration=time.time()-start_time

    if step %10==0:
        examples_per_sec=batch_size/duration
        sec_per_batch=float(duration)
        
        format_str=('step %d,loss=%.2f (%.1f examples/sec;%.3f sec/batch)')
        print(format_str% (step,loss_value,examples_per_sec,sec_per_batch))

num_examples=10000
import math
num_iter=int(math.ceil(num_examples/batch_size))
true_count=0
total_sample_count=num_iter*batch_size
step=0
while step <num_iter:
    image_batch,label_batch=sess.run([images_test,labels_test])
    predictions=sess.run([top_k_op],feed_dict={image_holder:image_batch,
                                               label_holder:label_batch})
    true_count+=np.sum(predictions)
    step+=1
    
precision=1.0*true_count/total_sample_count
print('precision @ 1=%.3f'%precision)
